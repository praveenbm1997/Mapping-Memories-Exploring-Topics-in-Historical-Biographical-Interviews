{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data file and Combining the data into paragrapgh by removing the shorter and noise in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered paragraphs have been processed and stored in memory.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# List of common filler words\n",
    "filler_words = {\"ja\", \"hm\", \"mhm\", \"ach\", \"gut\", \"und\", \"eben\", \"ne\", \"ok\", \"aha\", \"ach so\", \"nicht\", \"ja\", \"nein\", \"wieder\", \"schon\", \"naja\", \"wieso\", \"wieso nicht\", \"wieso nicht\"}\n",
    "\n",
    "# Filtering criteria\n",
    "min_words = 2\n",
    "min_unique_words = 2\n",
    "min_characters = 5\n",
    "\n",
    "def has_repeated_filler_patterns(sentence, filler_words):\n",
    "    \"\"\"\n",
    "    Check if any filler word appears repeatedly either consecutively\n",
    "    or separated by commas or spaces in the sentence.\n",
    "    \"\"\"\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(word) for word in filler_words) + r')\\b(?:[\\s,]+)+\\1\\b'\n",
    "    \n",
    "    if re.search(pattern, sentence):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_short_and_filler_sentences(text, filler_words, min_words, min_unique_words, min_characters):\n",
    "    \"\"\"\n",
    "    Filter sentences from the text based on length, unique words,\n",
    "    minimum characters, and absence of repeated filler patterns.\n",
    "    \"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    filtered_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        \n",
    "        if len(words) >= min_words and len(set(words)) >= min_unique_words and len(sentence) >= min_characters:\n",
    "            if not has_repeated_filler_patterns(sentence, filler_words):\n",
    "                filtered_sentences.append(sentence)\n",
    "    \n",
    "    return '. '.join(filtered_sentences)\n",
    "\n",
    "# File path\n",
    "file_path = 'Interviews_Dataset.xlsx'\n",
    "\n",
    "# Loading the Excel file\n",
    "excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Combining the sentences to paragraphs and applying the filtering\n",
    "combined_paragraphs = {}\n",
    "\n",
    "for sheet_name, df in excel_data.items():\n",
    "    # Drop the 'Timecode' and 'Sprecher' columns\n",
    "    df = df.drop(columns=['Timecode', 'Sprecher'])\n",
    "    \n",
    "    # Convert the 'Transkript' column to a single string\n",
    "    df['Transkript'] = df['Transkript'].astype(str)\n",
    "    paragraph = ' '.join(df['Transkript'].tolist())\n",
    "    \n",
    "    # Filter short sentences and apply filler pattern filtering\n",
    "    filtered_paragraph = filter_short_and_filler_sentences(paragraph, filler_words, min_words, min_unique_words, min_characters)\n",
    "    \n",
    "    # Save the filtered paragraph by sheet name in the dictionary\n",
    "    combined_paragraphs[sheet_name] = filtered_paragraph\n",
    "\n",
    "# The `combined_paragraphs` dictionary now contains the filtered paragraphs for each sheet in memory.\n",
    "# You can now use `combined_paragraphs` as needed in your code.\n",
    "\n",
    "print(\"Filtered paragraphs have been processed and stored in memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the preprocessed data into embedding model to genarate the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load the model Jina Embeddings\n",
    "model_name = 'jinaai/jina-embeddings-v2-base-de'\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Concatenate all filtered paragraphs into one large text string\n",
    "combined_text = ' '.join(combined_paragraphs.values())\n",
    "\n",
    "# Split the concatenated text into sentences\n",
    "sentences = combined_text.split('. ')\n",
    "\n",
    "batch_size = 32  \n",
    "embeddings = []\n",
    "\n",
    "# Process sentences in batches and compute embeddings\n",
    "for i in range(0, len(sentences), batch_size):\n",
    "    batch_sentences = sentences[i:i+batch_size]\n",
    "    inputs = tokenizer(batch_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).last_hidden_state\n",
    "        batch_embeddings = outputs.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "# Convert the embeddings list to a NumPy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the dimension of the embedded data for the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=20, n_components=10, min_dist=0.1, metric='cosine', low_memory=True)\n",
    "umap_embeddings = umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering the embedded data and saving in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 6\n",
      "Clusters have been saved to 'clusters.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "\n",
    "# Create a HDBSCAN Clustering instance with specified hyperparameters\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, cluster_selection_epsilon=0.4, metric='euclidean', cluster_selection_method='eom')\n",
    "clusters = clusterer.fit_predict(umap_embeddings)\n",
    "\n",
    "num_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "print(f\"Number of clusters: {num_clusters}\")\n",
    "# Create a list to hold clusters with their sentences\n",
    "clusters_list = []\n",
    "\n",
    "for cluster in np.unique(clusters):\n",
    "    # Convert the cluster ID to a regular Python int\n",
    "    cluster_id = int(cluster)\n",
    "    # Collect sentences for each cluster\n",
    "    sentences_in_cluster = [sentence for i, sentence in enumerate(sentences) if clusters[i] == cluster]\n",
    "    \n",
    "    # Create a dictionary for the cluster\n",
    "    cluster_dict = {\n",
    "        \"cluster_id\": cluster_id,\n",
    "        \"sentences\": sentences_in_cluster\n",
    "    }\n",
    "    \n",
    "    # Add the cluster dictionary to the list\n",
    "    clusters_list.append(cluster_dict)\n",
    "\n",
    "# Save the clusters to a JSON file\n",
    "output_json_file = 'clusters.json'\n",
    "with open(output_json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(clusters_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Clusters have been saved to '{output_json_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Debugging function to help track where issues might be occurring\n",
    "def debug_message(message):\n",
    "    print(f\"[DEBUG] {message}\")\n",
    "\n",
    "# Load the model and tokenizer once at the beginning\n",
    "try:\n",
    "    debug_message(\"Loading model and tokenizer...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", token=\"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\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", token=\"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\")\n",
    "    debug_message(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    debug_message(f\"Error loading model or tokenizer: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt function Helps to generate the topics from the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_topic(sentences):\n",
    "    try:\n",
    "        # Combine sentences into a single string\n",
    "        combined_sentences = \" \".join(sentences)\n",
    "        \n",
    "        # Create a prompt for the model\n",
    "        prompt = f\"\"\"Analysiere die folgenden S채tze und generiere einen pr채zisen, aussagekr채ftigen Themennamen von maximal 5 Worten. \\\n",
    "              Der Themenname sollte den Kerninhalt erfassen und spezifisch sein. \\\n",
    "                S채tze: {combined_sentences} Thema:\"\"\"\n",
    "        \n",
    "        # Tokenize input and generate the topic\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10, num_return_sequences=1)\n",
    "        \n",
    "        # Decode the generated topic\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        debug_message(f\"Error during topic generation: {str(e)}\")\n",
    "        return \"Topic generation failed\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Json file to feed the clustered data into LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clusters from the JSON file\n",
    "input_json_file = 'clusters.json'\n",
    "try:\n",
    "    debug_message(f\"Loading clusters from {input_json_file}...\")\n",
    "    with open(input_json_file, 'r', encoding='utf-8') as f:\n",
    "        clusters_list = json.load(f)\n",
    "    debug_message(\"Clusters loaded successfully.\")\n",
    "except Exception as e:\n",
    "    debug_message(f\"Error loading clusters: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# List to hold the results with generated topics\n",
    "clusters_with_topics = []\n",
    "\n",
    "def process_clusters_sequentially():\n",
    "    for cluster in clusters_list:\n",
    "        try:\n",
    "            # Extract cluster information\n",
    "            cluster_id = cluster[\"cluster_id\"]\n",
    "            sentences = cluster[\"sentences\"]\n",
    "            \n",
    "            # Skip the cluster with ID -1 (noise) and clusters with more than 500 sentences\n",
    "            if cluster_id == -1:\n",
    "                debug_message(f\"Skipping noise cluster {cluster_id}\")\n",
    "                continue\n",
    "            if len(sentences) > 500:\n",
    "                debug_message(f\"Skipping cluster {cluster_id} due to too many sentences ({len(sentences)}).\")\n",
    "                continue\n",
    "            \n",
    "            # Debug: Show the cluster info\n",
    "            debug_message(f\"Processing cluster {cluster_id} with {len(sentences)} sentences.\")\n",
    "            \n",
    "            # Generate a topic for the cluster\n",
    "            topic = generate_topic(sentences)\n",
    "            \n",
    "            # Add the generated topic to the cluster data\n",
    "            cluster[\"topic\"] = topic\n",
    "            \n",
    "            # Append the updated cluster to the results list\n",
    "            clusters_with_topics.append(cluster)\n",
    "            \n",
    "            # Clear memory after processing each cluster\n",
    "            torch.cuda.empty_cache()  # If you're using a GPU, clear the cache\n",
    "            \n",
    "            debug_message(f\"Processed cluster {cluster_id}, Topic: {topic}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            debug_message(f\"An error occurred while processing cluster {cluster_id}: {str(e)}\")\n",
    "\n",
    "# Process all clusters sequentially\n",
    "try:\n",
    "    process_clusters_sequentially()\n",
    "except Exception as e:\n",
    "    debug_message(f\"Error during cluster processing: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Save the clusters with topics to a new JSON file\n",
    "output_json_file = 'clusters_with_topics.json'\n",
    "try:\n",
    "    debug_message(f\"Saving clusters with topics to {output_json_file}...\")\n",
    "    with open(output_json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(clusters_with_topics, f, ensure_ascii=False, indent=4)\n",
    "    debug_message(\"Clusters with topics saved successfully.\")\n",
    "except Exception as e:\n",
    "    debug_message(f\"Error saving clusters: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation of clustered data in the main text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming 'clusters_with_topics' is already populated\n",
    "sentence_cluster_map = {}\n",
    "topic_map = {}\n",
    "\n",
    "# Populate sentence_cluster_map and topic_map with clusters and topics\n",
    "for cluster in clusters_with_topics:\n",
    "    cluster_id = cluster[\"cluster_id\"]\n",
    "    topic = cluster.get(\"topic\", f\"Cluster {cluster_id}\")\n",
    "    topic_map[cluster_id] = topic\n",
    "    for sentence in cluster[\"sentences\"]:\n",
    "        sentence_cluster_map[sentence] = cluster_id\n",
    "\n",
    "def generate_random_color():\n",
    "    return f\"#{random.randint(0,255):02x}{random.randint(0,255):02x}{random.randint(0,255):02x}\"\n",
    "\n",
    "# Create cluster colors\n",
    "cluster_colors = {-1: \"#000000\"}  # Noise points in black\n",
    "for cluster in set(sentence_cluster_map.values()):\n",
    "    if cluster != -1:\n",
    "        cluster_colors[cluster] = generate_random_color()\n",
    "\n",
    "def color_sentence(sentence, cluster):\n",
    "    color = cluster_colors[cluster]\n",
    "    cluster_id = sentence_cluster_map[sentence]\n",
    "    return f'<span class=\"sentence\" data-cluster=\"{cluster_id}\" style=\"color:{color}\">{sentence}</span>'\n",
    "\n",
    "# Create the colored text\n",
    "colored_text = []\n",
    "for sentence in sentence_cluster_map:\n",
    "    cluster = sentence_cluster_map[sentence]\n",
    "    colored_sentence = color_sentence(sentence, cluster)\n",
    "    colored_text.append(colored_sentence)\n",
    "\n",
    "html_output = '. '.join(colored_text)\n",
    "\n",
    "# Create the legend with topics\n",
    "cluster_counts = Counter(sentence_cluster_map.values())\n",
    "legend = '<div class=\"legend\"><h2>Cluster Legend</h2><ul>'\n",
    "for cluster, color in sorted(cluster_colors.items()):\n",
    "    count = cluster_counts[cluster]\n",
    "    if cluster == -1:\n",
    "        legend += f'<li style=\"color:{color}\">Noise: {count} sentences</li>'\n",
    "    else:\n",
    "        topic = topic_map.get(cluster, f\"Cluster {cluster}\")\n",
    "        legend += f'<li class=\"legend-item\" data-cluster=\"{cluster}\" style=\"color:{color}; cursor:pointer;\">{topic}: {count} sentences</li>'\n",
    "legend += '</ul></div>'\n",
    "\n",
    "# Add interactivity with JavaScript\n",
    "javascript = \"\"\"\n",
    "<script>\n",
    "    document.querySelectorAll('.legend-item').forEach(item => {\n",
    "        item.addEventListener('click', function() {\n",
    "            var cluster = this.getAttribute('data-cluster');\n",
    "            document.querySelectorAll('.sentence').forEach(sentence => {\n",
    "                if (sentence.getAttribute('data-cluster') === cluster) {\n",
    "                    sentence.style.backgroundColor = 'yellow';  // Highlight selected cluster sentences\n",
    "                } else {\n",
    "                    sentence.style.backgroundColor = '';  // Reset other sentences\n",
    "                }\n",
    "            });\n",
    "        });\n",
    "    });\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "# Save the HTML output to a file\n",
    "with open('colored_clusters_with_legend.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <style>\n",
    "            .legend {{ margin-bottom: 20px; }}\n",
    "            .legend-item {{ margin-bottom: 5px; }}\n",
    "            .sentence {{ padding: 2px; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        {legend}\n",
    "        <div class=\"text\">{html_output}</div>\n",
    "        {javascript}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
